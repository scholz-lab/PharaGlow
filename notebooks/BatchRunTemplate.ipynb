{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run Pharaglow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "# image io and analysis\n",
    "import json\n",
    "import pims\n",
    "import trackpy as tp\n",
    "\n",
    "# plotting\n",
    "import matplotlib  as mpl \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#our packages\n",
    "from pharaglow import tracking, run, features, util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### parameters for batch job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# this cell contains 'parameters' for batch jobs via papermill\n",
    "\n",
    "parameterfile = \"/home/nzjacic/Desktop/pharaglow_parameters_mks.txt\"\n",
    "inPath = \"/media/scholz_la/hd2/Nicolina/Raw_videos/10x_INF100\"\n",
    "outPath = \"/media/scholz_la/hd2/Nicolina/Raw_videos/10x_INF100\"\n",
    "lawnPath = \"/media/scholz_la/hd2/Nicolina/Lawns/\"\n",
    "movie = \"20200326_NZ0046\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "parameterfile = \"/home/mscholz/Desktop/Code/TestDataPGlow/exampleParameterfile.json\"\n",
    "inPath = \"/home/mscholz/Desktop/Code/TestDataPGlow/MS0006\"\n",
    "outPath = \"/home/mscholz/Desktop/Code/TestDataPGlow/MS0006\"\n",
    "lawnPath = \"None\"\n",
    "movie = \"MS0006\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parallelization parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nWorkers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load data and create binary masks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "movieID = movie[-6:]\n",
    "if lawnPath is not None and lawnPath != 'None':\n",
    "    lawnfile = os.path.join(lawnPath,movieID+'_lawn.tiff')\n",
    "else:\n",
    "    lawnfile = None\n",
    "\n",
    "\n",
    "# creating new file names\n",
    "fname = os.path.join(inPath,\"*.tiff\")\n",
    "outfile = os.path.join(outPath, movieID+\"_{}_{}.json\")\n",
    "print(outfile)\n",
    "saveparam = os.path.join(outPath, movieID+\"_parameters\")\n",
    "print(saveparam)\n",
    "\n",
    "# starting pharaglow\n",
    "print('Starting pharaglow analysis...')\n",
    "rawframes = pims.open(fname)\n",
    "rawframes = rawframes#[:2000]\n",
    "print('Loading parameters from {}'.format(parameterfile.split('/')[-2:]))\n",
    "with open(parameterfile) as f:\n",
    "    param = json.load(f)\n",
    "\n",
    "if lawnfile is not None and lawnfile != 'None':\n",
    "    print('open and binarize lawn file')\n",
    "    lawn = pims.open(lawnfile)[0]\n",
    "    binLawn = features.findLawn(lawn)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Improve lawn detection if neccessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if lawnfile is not None and lawnfile != 'None':\n",
    "    from skimage.filters import threshold_li, gaussian, threshold_yen, threshold_otsu\n",
    "    from skimage.morphology import skeletonize, watershed, disk, remove_small_holes, remove_small_objects\n",
    "    image = gaussian(lawn, 1, preserve_range = True)\n",
    "    thresh = threshold_otsu(image)\n",
    "    binary = image > thresh#*0.8\n",
    "    binary = remove_small_holes(binary, area_threshold=15000, connectivity=1, in_place=False)\n",
    "    binary = remove_small_objects(binary, min_size=50000, connectivity=8, in_place=False)\n",
    "    binLawn = binary\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(lawn)\n",
    "    plt.contour(binLawn, alpha=0.5, cmap='pink')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(binLawn)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create binary masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# detecting objects\n",
    "print('Binarizing images')\n",
    "masks = tracking.calculateMask(rawframes, minSize = param['minSize'], bgWindow = param['bgWindow']\n",
    "                               , thresholdWindow = param['thresholdWindow'], smooth =  param['smooth'],\n",
    "                               subtract =  param['subtract'], dilate = param['dilate'] , tfactor=param['tfactor'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure the thesholding worked otherwise change parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import label, regionprops\n",
    "t =100\n",
    "plt.figure(figsize=(18,16))\n",
    "plt.subplot(121)\n",
    "plt.imshow(rawframes[t])#+lawn)\n",
    "if lawnfile is not None:\n",
    "    plt.contour(binLawn, alpha=0.5, cmap='pink')\n",
    "plt.subplot(122)\n",
    "plt.imshow(masks[t])#[:600,1000:])#[500:1500,2000:3500])#[:,2500:])\n",
    "print(np.min(masks[t]))\n",
    "label_image = label(masks[t], background=0, connectivity = 1)\n",
    "for region in regionprops(label_image):\n",
    "    plt.text(region.centroid[1]+50, region.centroid[0], region.area, color ='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Detecting individual objects and tracking or use multiprocessing to speed up feature detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "if nWorkers ==1:\n",
    "\n",
    "    #masks = tracking.preprocess(rawframes, minSize = param['minSize'], threshold =None )\n",
    "    print('Detecting features')\n",
    "    features = tracking.runfeatureDetection(rawframes, masks, param, frameOffset = 0)\n",
    "else:\n",
    "    from multiprocessing import Pool\n",
    "    print('Detecting features')\n",
    "    def f(sl):\n",
    "        a,b = sl\n",
    "        if len(rawframes[a:b])>1:\n",
    "            return tracking.runfeatureDetection(rawframes[a:b+1], masks[a:b+1], param, frameOffset = a)\n",
    "    features = []\n",
    "    L = len(rawframes)\n",
    "    # create chunks of analysis based on how many workers we use\n",
    "    print(L)\n",
    "    chunksize = L//nWorkers//20\n",
    "    #slices = np.arange(L)\n",
    "    slices = zip((range(0,L, chunksize)), (range(chunksize,L+chunksize, chunksize)))\n",
    "    \n",
    "    p = Pool(processes = nWorkers)\n",
    "    start = time.time()\n",
    "    for k, res in enumerate(p.imap_unordered(f, slices)):\n",
    "        features.append(res)\n",
    "        if k == nWorkers:\n",
    "            print('Expected time is approx. {} s'.format((L/chunksize-k)*(time.time()-start)/nWorkers/2))\n",
    "        #print(p, time.time()-start)\n",
    "    features = pd.concat(features)\n",
    "    p.close()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # trajectories\n",
    "features.head(5)\n",
    "features.info(memory_usage='deep')\n",
    "features.to_json(outfile.format('features', 'all'), orient='split')\n",
    "\n",
    "# parameter file\n",
    "shutil.copyfile(parameterfile, saveparam, follow_symlinks=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = pd.read_json(outfile.format('features', 'all'), orient='split', numpy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Link objects to trajectories and interpolate short misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Done')\n",
    "print('Linking trajectories')\n",
    "#pred = tp.predict.NearestVelocityPredict()\n",
    "#trajectories = pred.link_df(features,param['searchRange'], memory = param['memory'])\n",
    "trajectories = tp.link_df(features,param['searchRange'], memory = param['memory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Interpolating trajectories')\n",
    "traj = []\n",
    "for particle_index in trajectories['particle'].unique():\n",
    "    tmp = trajectories[trajectories.loc[:,'particle'] == particle_index]\n",
    "    # make new columns for the offset\n",
    "    traj.append(tracking.interpolateTrajectories(tmp))\n",
    "trajectories = pd.concat(traj, ignore_index = True)\n",
    "trajectories['shapeX'] = trajectories['shapeX'].astype(int)\n",
    "trajectories['shapeY'] = trajectories['shapeY'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(trajectories['particle'].nunique())\n",
    "trajectories = tp.filter_stubs(trajectories,param['minimalDuration'])\n",
    "print(trajectories['particle'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### add the missing images to interpolated trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Fill in missing images')\n",
    "from pharaglow import tracking, run, features\n",
    "# interpolate the shape parameter\n",
    "\n",
    "trajectories[['image', 'slice']] = trajectories.apply(\\\n",
    "       lambda row: pd.Series(tracking.fillMissingImages(rawframes, row['frame'], row['x'], row['y'],\\\n",
    "                                               lengthX=row['shapeX'],lengthY=row['shapeY'], size=param['watershed'])) if np.all(np.isnan(row['image'])) else pd.Series([row['image'], row['slice']]), axis=1)\n",
    "trajectories[['diffI']] = trajectories.apply(\\\n",
    "        lambda row: pd.Series(tracking.fillMissingDifferenceImages(rawframes, row['frame'], row['x'], row['y'],\\\n",
    "                                                lengthX=row['shapeX'],lengthY=row['shapeY'], size=param['watershed'])) if np.all(np.isnan(row['diffI'])) else row['diffI'], axis=1)\n",
    "# trajectories[['diffI']] = trajectories.apply(\\\n",
    "#         lambda row: pd.Series(tracking.fillMissingDifferenceImages(rawframes, row['frame'], row['x'], row['y'],\\\n",
    "#                                                 lengthX=row['shapeX'],lengthY=row['shapeY'], size=param['watershed'])) if np.sum((row['diffI']))==0 else row['diffI'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Extract lawn info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def inside(x,y,binLawn):\n",
    "    return binLawn[int(y), int(x)]\n",
    "\n",
    "if lawnfile is not None:\n",
    "    trajectories['inside'] = trajectories.apply(\\\n",
    "        lambda row: pd.Series(inside(row['x'], row['y'], binLawn)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show resulting trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,11))\n",
    "tp.plot_traj(trajectories, superimpose=1-masks[500]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check slow-down before continuing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lawnfile is not None:\n",
    "    plt.figure(figsize=(12,8))\n",
    "    vcut = []\n",
    "    dt = 1000\n",
    "    for pid in trajectories['particle'].unique():\n",
    "        tmp = trajectories[['frame', 'x', 'y']][trajectories.loc[:,'particle'] == pid].diff()\n",
    "        f = (trajectories[['inside']][trajectories.loc[:,'particle'] == pid]).mean().values\n",
    "        if f<0.9 and f>0.01:\n",
    "            t0 = np.where((trajectories[['inside']][trajectories.loc[:,'particle'] == pid])==1)[0][0]\n",
    "            print('t0:', t0)\n",
    "            try:\n",
    "                if t0>dt:\n",
    "                    print('pid:', pid)\n",
    "                    time = np.linspace(0,2*dt/30., 2*dt)\n",
    "                    #print('time:', len(time))\n",
    "                    v = np.sqrt((tmp['x']**2+tmp['y']**2))/tmp['frame']*30*2.4\n",
    "                    #print('v:', v)\n",
    "                    #print('v.iloc:', v.iloc[t0-dt:t0+dt])\n",
    "                    plt.plot(time, v.iloc[t0-dt:t0+dt], 'navy', alpha=0.1)\n",
    "                    vcut.append(v.iloc[t0-dt:t0+dt].values)\n",
    "                else:\n",
    "                    print('trajectory is too short')\n",
    "            except ValueError:\n",
    "                print('t0-dt or t0+dt exceeds number of frames')\n",
    "                continue\n",
    "                    \n",
    "    plt.plot(time, np.mean(np.array(vcut), axis=0), color='navy')\n",
    "    plt.plot(time, util.smooth(np.mean(np.array(vcut), axis=0), 30), color='r')\n",
    "    plt.axvline(dt/30, color='k', linestyle='--')\n",
    "    plt.ylabel(r\"velocity ($\\mu$m/s)\");\n",
    "    plt.xlabel(\"time (s)\");\n",
    "    plt.ylim(0,150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write trajectories to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # write trajectories to separate files.\n",
    "for particle_index in trajectories['particle'].unique():\n",
    "    tmp = trajectories[trajectories.loc[:,'particle'] == particle_index]\n",
    "    tmp.to_json(outfile.format('trajectories', int(particle_index)), orient='split')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### run the whole pharaglow feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "def parallelize_dataframe(df, func, params, n_cores):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    print([len(d) for d in df_split])\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.starmap(func, zip(df_split, np.repeat(params, len(df_split)))))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### run this if acidentally used an odd length like an idiot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def padIm(im, size = 22500):\n",
    "    return np.pad(im, (0,size-len(im)), mode='constant', constant_values=0)\n",
    "\n",
    "def fixImages(trajectories):\n",
    "    trajectories[['image']] = trajectories.apply(\\\n",
    "       lambda row: pd.Series(padIm(row['image'], row['shapeX']*row['shapeY'])) if len(row['image'])<row['shapeX']*row['shapeY'] else row['image'], axis=1)\n",
    "    trajectories[['diffI']] = trajectories.apply(\\\n",
    "       lambda row: pd.Series(padIm(row['diffI'], row['shapeX']*row['shapeY'])) if len(row['diffI'])<row['shapeX']*row['shapeY'] else row['diffI'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Save data as json format (every trajectory in a file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "path = os.path.dirname(outfile)\n",
    "for fn in os.listdir(path):\n",
    "    file = os.path.join(path,fn)\n",
    "    if os.path.isfile(file) and 'trajectories_' in fn and fn.endswith('.json'):\n",
    "        particle_index = int(fn.split('.')[0].split('_')[-1])\n",
    "        traj =  pd.read_json(file, orient='split', numpy = True)\n",
    "        traj['shapeX'] = traj['shapeX'].astype(int)\n",
    "        print('Analyzing trajectory:', fn)\n",
    "        # interpolate if images are not the right size:\n",
    "        fixImages(traj)\n",
    "        #tmp = run.runPharaglowOnStack(traj, param)\n",
    "        tmp = parallelize_dataframe(traj, run.runPharaglowOnStack, n_cores = nWorkers, params = param)\n",
    "        # get more exact entry location\n",
    "        if lawnfile is not None:\n",
    "            tmp['insideHead'] = tmp.apply(\\\n",
    "                lambda row: pd.Series(features.headLocationLawn(row['Centerline'],row['slice'], binLawn)), axis=1)\n",
    "            tmp['insideHeadIntensity'] = tmp.apply(\\\n",
    "                lambda row: pd.Series(features.headLocationLawn(row['Centerline'],row['slice'], lawn)), axis=1)\n",
    "        tmp.to_json(outfile.format('results', particle_index), orient='split')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "papermill": {
   "duration": 0.011375,
   "end_time": "2020-05-04T13:03:57.031896",
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/mscholz/Desktop/Code/PharaGlow/notebooks/BatchRunTemplate.ipynb",
   "output_path": "/home/mscholz/Desktop/Code/PharaGlow/notebooks/BatchRunTemplate.ipynb",
   "parameters": {
    "inPath": "/home/mscholz/Desktop/Code/TestDataPGlow/MS0006",
    "lawnPath": "None",
    "movie": "MS0006",
    "outPath": "/home/mscholz/Desktop/Code/TestDataPGlow/MS0006",
    "parameterfile": "/home/mscholz/Desktop/Code/TestDataPGlow/exampleParameterfile.json"
   },
   "start_time": "2020-05-04T13:03:57.020521",
   "version": "2.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
