{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pharaglow Analysis Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import timeit\n",
    "from datetime import date\n",
    "# image io and analysis\n",
    "import json\n",
    "import pims\n",
    "from skimage.io import imsave\n",
    "import trackpy as tp\n",
    "\n",
    "# plotting\n",
    "import matplotlib  as mpl \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#our packages\n",
    "import pharaglow\n",
    "from pharaglow import tracking, run, features, util, io, extract\n",
    "\n",
    "# show logger messabes loally\n",
    "import logging\n",
    "logging.debug('pharaglow')\n",
    "# reduce the output messages from trackpy\n",
    "tp.quiet(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs the pharaglow image analysis pipeline. It comprises three stages on analysis which can be done sequentially and are independent. Analyses can be interrupted at the end of each stage after saving the output dataframe. \n",
    "\n",
    "**1. Step -  Basic object detection**\n",
    "    This step creates a \"_features.json\" file which contains a table of objects detected in each frame.\n",
    "    Beyond finding the center of mass of an object, no further image analysis is done here.\n",
    "    \n",
    "**2. Step - Linking objects into trajectories**\n",
    "    This results in individual files \"_trajectory.json\" for each tracked animal.\n",
    "    \n",
    "**3. Step - Analysing the details of object shapes**\n",
    "    This step is doing the heavy lifting: It extracts centerlines, widths, contours and other object descriptors from the objects\n",
    "\n",
    "All subsequent analyses steps add 'columns' to the data, and thus features is a subset of trajectories is a subset of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pims.pipeline\n",
    "def squeeze(im):\n",
    "    return np.squeeze(im)\n",
    "\n",
    "def setup(parameterfile, inPath, outPath, movie, lawnPath=None):\n",
    "    \"\"\"helper function to simplify the setting up before analysis. Handles path checking and creating, parameter reads ad data reads.\"\"\"\n",
    "    npaths = {'parameter file': parameterfile,\n",
    "          'inPath':inPath,\n",
    "          'outPath': outPath}\n",
    "       \n",
    "    ### define the date with the format YYYYMMDD\n",
    "    month = date.today().month\n",
    "    if len(str(month))==1:\n",
    "        month = f'0{str(month)}'\n",
    "    day = date.today().day\n",
    "    if len(str(day))==1:\n",
    "        day = f'0{str(month)}'\n",
    "    today  = f\"{date.today().year}{month}{day}\" \n",
    "      \n",
    "    ### start a logger\n",
    "    logger = io.log_setup('PharaGlow', 10, fname = os.path.join(outPath, f'{today}_{movie}_pharaglow_log.txt'))\n",
    "    logger.info(f\"today is {today}\")\n",
    "    \n",
    "    \n",
    "    ### create filenames\n",
    "    fname = os.path.join(inPath,\"*.tif*\")\n",
    "    outfile = os.path.join(outPath, movie+\"_{}_{}.json\")\n",
    "    imfile =  os.path.join(outPath, movie+\"_{}_{}.tiff\")\n",
    "    saveparam = os.path.join(outPath, movie+\"_parameters\")\n",
    "    \n",
    "    logger.info(f\"parameters file will be saved as {saveparam}\")\n",
    "    logger.info(f\"output file will be saved as {outfile}\")\n",
    "    logger.info(f\"image files will be saved as {imfile}\")\n",
    "    \n",
    "    # check if all the paths exist\n",
    "    for key, value in npaths.items():    \n",
    "        if os.path.exists(value):\n",
    "            logger.info(f'{key}: {value}')\n",
    "        else:\n",
    "            logger.warning(f\"Warning! The path for {key} doesnt exist: {value}\")\n",
    "    logger.info(f\"Loading parameters from {parameterfile}...\")\n",
    "    \n",
    "    ### load analysis parameters\n",
    "    with open(parameterfile) as f:\n",
    "        param = json.load(f)\n",
    "    logger.info(f\"parameters file loaded as 'param':{param}\")\n",
    "    # save a copy of the parameters\n",
    "    logger.info(\"Saving parameters...\")\n",
    "    with open(saveparam, 'w') as f:\n",
    "        json.dump(param, f)\n",
    "    logger.info(f\"parameters saved as {parameterfile}\")\n",
    "    \n",
    "    ### load lawns\n",
    "    if lawnPath is not None and lawnPath != 'None':\n",
    "        try:\n",
    "            lawnfile = os.path.join(lawnPath,movie+'_lawn.tiff')\n",
    "            lawn = pims.open(lawnfile)[0]\n",
    "            binLawn = features.findLawn(lawn)\n",
    "        except Exception:\n",
    "            lawnfile = os.path.join(lawnPath,movie+'_lawn.bmp')\n",
    "            lawn = pims.open(lawnfile)[0]\n",
    "            binLawn = features.findLawn(lawn)\n",
    "        logger.info(\"Lawnfile opened as 'lawn'\")\n",
    "    else:\n",
    "        lawn = None\n",
    "        \n",
    "    ### load images\n",
    "    start = timeit.default_timer()\n",
    "    logger.info(\"Loading tiff files.\")\n",
    "    rawframes = pims.open(fname)\n",
    "    # in case we get accidental rgb\n",
    "    if len(rawframes[0].shape)>2:\n",
    "        rawframes = squeeze(rawframes)\n",
    "    stop = timeit.default_timer()\n",
    "    logger.info(f\"image loading time: {stop - start}s\")  \n",
    "    nfiles = len([f for f in os.listdir(inPath) if '.tif' in f])\n",
    "    # tiff files\n",
    "    logger.info(f\"Number of tiff files: {nfiles}\")\n",
    "    # rawframes \n",
    "    logger.info(f\"Number of rawframes: {len(rawframes)}\")\n",
    "\n",
    "    if nfiles != len(rawframes):\n",
    "        logger.warning(\"the number of tiff files doesn't match with the number of rawframes !\")\n",
    "\n",
    "    return logger, param, rawframes, lawn, outfile, imfile\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "parameterfile = r\"/gpfs/soma_fs/home/valerio/pharaglow/AnalysisParameters_1x.json\"\n",
    "inPath = r\"/gpfs/soma_fs/home/valerio/pharaglow/demo_data\"\n",
    "outPath = r\"/gpfs/soma_fs/home/valerio/pharaglow/output_demo\"\n",
    "movie = \"Basler_acA3088-57um__22844917__20200318_150020103\"\n",
    "\n",
    "import os\n",
    "nWorkers = len(os.sched_getaffinity(0))\n",
    "print(f\"Workers (CPU cores): {nWorkers}\")\n",
    "\n",
    "depth = 'uint8'\n",
    "save_minimal = True\n",
    "\n",
    "lawnPath = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger, param, rawframes, lawn, outfile, imfile = setup(parameterfile, inPath, outPath, movie, lawnPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create binary masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "# detecting objects\n",
    "logger.info('Binarizing images...')\n",
    "\n",
    "masks = tracking.calculateMask(rawframes,\n",
    "                               bgWindow = param['bgWindow'],\n",
    "                               thresholdWindow = param['thresholdWindow'],\n",
    "                               smooth =  param['smooth'],\n",
    "                               subtract =  param['subtract'],\n",
    "                               dilate = param['dilate'],\n",
    "                               tfactor=param['tfactor'])\n",
    "\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "logger.info(f\"binary masks created ({stop - start}s)\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure the thresholding worked otherwise change parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a rawframe to visualize\n",
    "t = 5400 \n",
    "\n",
    "if t> (len(rawframes)-1):\n",
    "    # Check if the selected rawframe is present otherwise t=0\n",
    "    print(f\"Warning ! Max {len(rawframes)} rawframes. {t} changed to 0\")\n",
    "    t=0\n",
    "\n",
    "print(f\"rawframe {t} to visualize \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "plt.subplot(121)\n",
    "# Plot the histogram of the pixel intensity values of the rawframe\n",
    "plt.hist(rawframes[t].ravel(), bins=256, log=True)\n",
    "plt.xlim(0, 260) # xlim for 8 bits image\n",
    "\n",
    "plt.subplot(122)\n",
    "# Adjust the color limit for the rawframe for vizualisation only\n",
    "color = (0,150) # 0<=color<=255 for 8 bits image\n",
    "# color = None \n",
    "plt.imshow(rawframes[t],clim = color)\n",
    "plt.colorbar(orientation='horizontal')\n",
    "plt.title(f\"{movie} at frame {t}\", fontsize=14)  \n",
    "\n",
    "# Save\n",
    "fig_path = os.path.join(outPath,f'{date.today()}_{movie}_frame{t}_hist.pdf')\n",
    "plt.savefig(fig_path)\n",
    "print(\"figure saved as\", fig_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show the mask and detected objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib qt\n",
    "\n",
    "from skimage.measure import label, regionprops\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(121)\n",
    "# Show the rawframe\n",
    "plt.imshow(rawframes[t],clim= color)#+lawn)\n",
    "if lawn is not None:\n",
    "    # Show the lawn\n",
    "    plt.contour(binLawn, alpha=0.5, cmap='pink')\n",
    "    \n",
    "plt.subplot(122)\n",
    "# Show the masks and their size [px]\n",
    "plt.imshow(masks[t])#[:600,1000:])#[500:1500,2000:3500])#[:,2500:])\n",
    "# print(np.min(masks[t]))\n",
    "label_image, num = label(masks[t], background=0, connectivity = 1,return_num=True)\n",
    "print(f\"{num} detected objects\")\n",
    "for region in regionprops(label_image):\n",
    "    plt.text(region.centroid[1]+50, region.centroid[0], region.area, color ='w')\n",
    "\n",
    "plt.suptitle(f\"Rawframe and masks (#{num}) rawframe {t} ({movie})\", fontsize=14)    \n",
    "plt.tight_layout()\n",
    "\n",
    "# save the pdf\n",
    "fig_path = os.path.join(outPath,f'{date.today()}_{movie}_frame{t}_masks.pdf')\n",
    "plt.savefig(fig_path)\n",
    "print(\"figure saved as\", fig_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting individual objects and tracking or use multiprocessing to speed up feature detection\n",
    "\n",
    "This section will go through all frames and find worm-sized (as specified by the parameters) objects. It creates a pd.Dataframe containing these and a stack of images (numpy array) that contain a cropped area around each worm. Note: Each worm image will be length x length x 8bit. So with 30 worms per image you expect the image array to be 6Gb/10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "logger.info('Detecting features...')\n",
    "logger.info(f'...with {nWorkers} workers')\n",
    "objects, images = util.parallel_analysis((masks, rawframes), param, tracking.parallelWorker, framenumbers = None, nWorkers = nWorkers, output= None, depth = depth)\n",
    "# create a link between image and dataframe\n",
    "objects['im_idx'] = np.arange(len(objects))\n",
    "stop = timeit.default_timer()\n",
    "logger.info(f\"features detected ({stop - start}s)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files monitoring\n",
    "logger.info(f\" Number of frames in features:{objects['frame'].nunique()}\")\n",
    "                                                       \n",
    "if len(rawframes) != len(objects['frame'].unique()):\n",
    "    logger.warning(f\" Number of frames in features ({objects['frame'].nunique()}) and the number of rawframes ({len(rawframes)}) don't match !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize results of object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show the area of all objects\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "objects['area'].hist(bins = 30)\n",
    "plt.xlabel('Area (px)')\n",
    "plt.subplot(122)\n",
    "objects['frame'].value_counts().sort_index().plot()\n",
    "plt.ylabel('Number of objects')\n",
    "plt.xlabel('Frame')\n",
    "\n",
    "# save the pdf\n",
    "plt.title(f\"{movie}\", fontsize=24)\n",
    "plt.savefig(os.path.join(outPath,f'{date.today()}_{movie}_objects_.pdf'))\n",
    "\n",
    "logger.info(f\"features.area.min():{objects.area.min()}\") # region.area > params['minSize']\n",
    "logger.info(f\"features.area.max():{objects.area.max()}\") # region.area < params['maxSize']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save features and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "# saving features\n",
    "logger.info(\"Saving features...\")\n",
    "objects.info(memory_usage='deep')\n",
    "objects.to_json(outfile.format('features', 'all'), orient='split')\n",
    "stop = timeit.default_timer()\n",
    "logger.info(f\"features saved as {outfile.format('features', 'all')} ({stop - start}s)\")\n",
    "\n",
    "start = timeit.default_timer()\n",
    "# saving images\n",
    "imsave(imfile.format('images', 'all'), images)\n",
    "stop = timeit.default_timer()\n",
    "logger.info(f\"images saved as {imfile.format('images', 'all')} ({stop - start}s)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Load features and images if continuing prior analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# leaving this here for re-analysis\n",
    "if False:\n",
    "    # Load feature\n",
    "    start = timeit.default_timer()\n",
    "    logger.info(\"Loading features...\")\n",
    "    objects = io.load(outfile.format('features', 'all'), orient='split')\n",
    "    images = pims.open(imfile.format('images', 'all'))\n",
    "    stop = timeit.default_timer()\n",
    "    logger.info(f\"features loaded ({stop - start}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link objects to trajectories using trackpy and interpolate short misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Linking trajectories...')\n",
    "logger.info(f\"Parameter searchRange: {param['searchRange']} px\")\n",
    "logger.info(f\"Parameter memory: {param['memory']} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred = tp.predict.NearestVelocityPredict()\n",
    "#trajectories = pred.link_df(features,param['searchRange'], memory = param['memory'])\n",
    "trajectories = tp.link_df(objects,param['searchRange'], memory = param['memory'])\n",
    "logger.info(f\"Number of trajectories after linking: {len(trajectories['particle'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the variable features to save memory\n",
    "del objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "tp.plot_traj(trajectories, colorby = 'particle', superimpose=1-masks[t],label=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Filtering out trajectories which last less than the minimal duration ({param['minimalDuration']} frames)...\")\n",
    "logger.info(f\"Nb of trajectories before filtering: {trajectories['particle'].nunique()}\")\n",
    "\n",
    "trajectories = tp.filter_stubs(trajectories,param['minimalDuration'])\n",
    "logger.info(f\"Nb of trajectories after filtering: {trajectories['particle'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = tp.plot_traj(trajectories, colorby = 'particle', superimpose=1-masks[t],label=False);\n",
    "# save the pdf\n",
    "ax.set_title(f\"{movie}\", fontsize=24)\n",
    "fig.savefig(os.path.join(outPath,f'{date.today()}_{movie}_trajectories_filtered.pdf'))\n",
    "\n",
    "# with labels\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = tp.plot_traj(trajectories, colorby = 'particle', superimpose=1-masks[t],label=True);\n",
    "# save the pdf\n",
    "ax.set_title(f\"{movie}\", fontsize=24)\n",
    "fig.savefig(os.path.join(outPath,f'{date.today()}_{movie}_trajectories_filtered_labelled.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save individual trajectories & add the missing images to interpolated trajectories\n",
    "\n",
    "Here we do multiple things: Add missing rows to the trajectory, create a separate image stack for each animal and save the trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract lawn info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inside(x,y,binLawn):\n",
    "    return binLawn[int(y), int(x)]\n",
    "\n",
    "if lawn is not None:\n",
    "    trajectories['inside'] = trajectories.apply(\\\n",
    "        lambda row: pd.Series(inside(row['x'], row['y'], binLawn)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Saving {trajectories['particle'].nunique()} trajectories to separate files...\")\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "for particle_index in trajectories['particle'].unique():\n",
    "    tmp = trajectories[trajectories.loc[:,'particle'] == particle_index].copy()\n",
    "    ims = images[tmp['im_idx']]\n",
    "    ims = np.array(ims, dtype = 'uint8')\n",
    "    # generate an interpolated trajectory where all frames are accounted for\n",
    "    traj_interp, ims_interp = tracking.interpolate_helper(rawframes, ims, tmp, param)\n",
    "    # save the new single worm movie\n",
    "    imsave(imfile.format('images', particle_index), np.array(ims_interp, dtype='uint8'))\n",
    "    # add some basic image properties\n",
    "    traj_interp = features.calculateImageproperties(traj_interp, ims_interp)\n",
    "    # save the trajectory\n",
    "    traj_interp.to_json(outfile.format('trajectories', int(particle_index)), orient='split')\n",
    "    \n",
    "stop = timeit.default_timer()\n",
    "logger.info(f\"trajectories saved as json files ({stop - start}s)\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run the whole pharaglow feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "# save only minimal outputs - reduces save by approx factor 3\n",
    "\n",
    "# analyze all trajectories\n",
    "for fn in os.listdir(outPath):\n",
    "    file = os.path.join(outPath,fn)\n",
    "    \n",
    "    if os.path.isfile(file) and f'{movie}_trajectories_' in fn and fn.endswith('.json'):\n",
    "        particle_index = int(fn.split('.')[0].split('_')[-1])\n",
    "        traj =  io.load(file, orient='split')\n",
    "        # load images\n",
    "        images = pims.open(imfile.format('images', particle_index))\n",
    "        if len(traj.index)<1:\n",
    "            print('Skipped', file)\n",
    "            continue\n",
    "        logger.info('Analyzing trajectory:%s', fn)\n",
    "        \n",
    "        tmp,_ = util.parallel_analysis((images,), param,\\\n",
    "                          parallelWorker= run.parallel_pharaglow_run, framenumbers = traj['frame'], nWorkers = nWorkers, output= None)\n",
    "        # add second area calculation\n",
    "        tmp['Area2'] = [np.sum(mask) for mask in tmp['Mask']]\n",
    "\n",
    "        # remove some columns to make the result smaller\n",
    "        if save_minimal:\n",
    "            tmp = tmp.drop(['Mask', 'SkeletonX', 'SkeletonY', 'ParX', 'ParY', \n",
    "                            'Xstart', 'Xend', 'dCl', 'Widths', 'Contour', 'Gradient', \n",
    "                            'Kymo', 'KymoGrad', 'Similarity', 'Xtmp'], axis = 1, errors = 'ignore')\n",
    "        # add the basic tracker info - you can also keep these as separate files\n",
    "        tmp = tmp.merge(traj, on='frame', how = 'outer')\n",
    "        # drop nans to allow post processing\n",
    "        tmp = tmp.dropna(how='all')\n",
    "        tmp = tmp[tmp['Straightened'].notna()]\n",
    "        print(tmp.info())\n",
    "        # run some stuff on the whole dataframe.\n",
    "        run.pharynxorientation(tmp)\n",
    "        # extract pumps\n",
    "        tmp[['pumps']] = tmp.apply(\\\n",
    "        lambda row: pd.Series(features.extractPump(row['Straightened'])), axis=1)\n",
    "        # get more exact entry location\n",
    "        if lawn is not None:\n",
    "            tmp['insideHead'] = tmp.apply(\\\n",
    "                lambda row: pd.Series(features.headLocationLawn(row['Centerline'],row['slice'], binLawn)), axis=1)\n",
    "            tmp['insideHeadIntensity'] = tmp.apply(\\\n",
    "                lambda row: pd.Series(features.headLocationLawn(row['Centerline'],row['slice'], lawn)), axis=1)\n",
    "        \n",
    "        tmp.to_json(outfile.format('results', particle_index), orient='split')\n",
    "        \n",
    "if save_minimal:\n",
    "    logger.info('minimal information saved')\n",
    "    \n",
    "stop = timeit.default_timer()\n",
    "logger.info(f\"Whole pharaglow features extracted ({stop - start}s)\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if data has been analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files monitoring\n",
    "files_list = os.listdir(outPath)\n",
    "f1 =[]\n",
    "f2 =[]\n",
    "\n",
    "path = os.path.dirname(outfile)\n",
    "\n",
    "for fn in files_list:\n",
    "    file = os.path.join(path,fn)\n",
    "    if os.path.isfile(file) and f'{movie}_trajectories_' in fn  and fn.endswith('.json'):\n",
    "        if not 'all' in fn: \n",
    "            particle_index = int(fn.split('.')[0].split('_')[-1])\n",
    "            f1.append(particle_index)\n",
    "    if os.path.isfile(file) and f'{movie}_results_' in fn and fn.endswith('.json'): \n",
    "        particle_index = int(fn.split('.')[0].split('_')[-1])\n",
    "        f2.append(particle_index)\n",
    "\n",
    "logger.info('trajectories.json files: %s ', len(f1))\n",
    "logger.info('results.json files: %s ', len(f2))\n",
    "if len(f1) != len(f2):\n",
    "    logger.warning('trajectories - results: %s', set(f1).symmetric_difference(set(f2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"PharaGlow ends here\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pharaglow-pip",
   "language": "python",
   "name": "pharaglow-pip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
