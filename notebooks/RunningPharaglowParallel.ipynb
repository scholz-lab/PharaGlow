{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Pharaglow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %matplotlib qt\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import timeit\n",
    "\n",
    "# image io and analysis\n",
    "import json\n",
    "import pims\n",
    "import trackpy as tp\n",
    "\n",
    "# plotting\n",
    "import matplotlib  as mpl \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#our packages\n",
    "from pharaglow import tracking, run, features, util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CHANGE FILE/DIRECTORY NAMES\n",
    "parameterfile = \"../AnalysisParameters_1x\"\n",
    "inPath = \"/home/mscholz/Dropbox (Scholz Lab)/Shared/Data/MS0006_0_1000frames\"\n",
    "outPath = \"/home/mscholz/Desktop/TestOutput_MS0006/\"\n",
    "lawnPath = None #\"/opt/data/Lawns/\"\n",
    "movie = \"MS0006_0_1000frames\"\n",
    "movieID = movie[-6:]\n",
    "nWorkers = 5\n",
    "\n",
    "# # Write the path of the directories where\n",
    "# # > is the parameters file\n",
    "# parameterfile = '/home/ebonnard/Desktop/Elsa/2_PharaGlow/20201217_solenoid_mutants/EB0068/Linking/EB0068_parameters.txt'\n",
    "# # > are the tiff files\n",
    "# inPath = '/home/nif/Desktop/data/Elsa/1_Rawdata/20201217_solenoid_mutants/recording/EB0068c/'\n",
    "# # > will be saved the pharaglow output files\n",
    "# outPath = f'/home/ebonnard/Desktop/Elsa/2_PharaGlow/20201217_solenoid_mutants/EB0068/test'\n",
    "# # > is the tiff file with the bacterial lawn (if no lawn: None)\n",
    "# lawnPath = None #\"/opt/data/Lawns/\"\n",
    "\n",
    "# # Set the names of the tiff files folder and the identification number of the recording\n",
    "# movie = \"EB0068\"\n",
    "# movieID = movie # the ID should be AA1000\n",
    "# # movieID = movie[-6:]\n",
    "\n",
    "# # Set the number of processing cores used for the analysis\n",
    "# nWorkers = 20\n",
    "\n",
    "# # Inactivate (False) or activate (True) the debug mode\n",
    "debug=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check paths\n",
    "\n",
    "npaths = {'parameter file': parameterfile,\n",
    "          'inPath':inPath,\n",
    "          'outPath': outPath}\n",
    "\n",
    "for key, value in npaths.items():    \n",
    "    if os.path.exists(value):\n",
    "        print(f'{key}: {value}')\n",
    "    else:\n",
    "        print(f\"Warning! The path for {key} doesnt exist: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_setup(name, level, fname):\n",
    "    '''This function will setup a logger with the name and level you pass as input'''\n",
    "    \n",
    "    # start a logger\n",
    "    logger = logging.getLogger(name)\n",
    "    # set a formatter to manage the output format of our handler\n",
    "    formatter = logging.Formatter('%(asctime)s | %(name)s |  %(levelname)s: %(message)s')\n",
    "    \n",
    "    # set the level passed as input, has to be logging.LEVEL not a string\n",
    "    # until we do so mylog doesn't have a level and inherits the root logger level:WARNING\n",
    "    logger.setLevel(level)\n",
    "\n",
    "    # add a handler to send INFO level messages to console\n",
    "    # console_handler = logging.StreamHandler()\n",
    "    # console_handler.setLevel(logging.INFO)\n",
    "    # logger.addHandler(console_handler)  \n",
    "    \n",
    "    # add a handler to send DEBUG level messages to file\n",
    "    # all you need is a file name I added the 'w' so each time a new file will be created\n",
    "    # without it the messagges will be appended to the same file\n",
    "    file_handler = logging.FileHandler(fname)\n",
    "    # file_handler = logging.FileHandler(fname,'w')\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    # return the logger object\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date of the analysis\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "today = today.strftime(\"%Y%m%d\")\n",
    "print(f'today is {today}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# # remember to uncomment these lines if you have want to set your own root logger (use once per script)\n",
    "# for handler in logging.root.handlers[:]:\n",
    "#    logging.root.removeHandler(handler)\n",
    "# # We disabled the existing handlers after import logging so we can configure the root logger as we want\n",
    "\n",
    "# We set the level to root logger level to WARNING\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# set the path and the name of the log\n",
    "fname = os.path.join(outPath, f'{today}_{movieID}_pharaglow_log.txt')\n",
    "\n",
    "# set the logger\n",
    "logger = log_setup('PharaGlow', logging.DEBUG, fname)\n",
    "logger.info(f'The log file is stored as {fname}')\n",
    "\n",
    "\n",
    "#  Source: https://climate-cms.org/2018/10/05/introduction-to-python-logging.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new file names\n",
    "fname = os.path.join(inPath,\"*.tiff\")\n",
    "outfile = os.path.join(outPath, movieID+\"_{}_{}.json\")\n",
    "logger.info(f\"output file would be saved as {outfile}\")\n",
    "saveparam = os.path.join(outPath, movieID+\"_parameters\")\n",
    "logger.info(f\"paramaters file would be saved as {saveparam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    \n",
    "    logger.debug('Select tiff files to analyze:')\n",
    "    \n",
    "    first_tiff = 1 # tiff file name/number (minimum=1)\n",
    "    last_tiff = 200\n",
    "\n",
    "    n = np.arange(first_tiff-1, last_tiff)  \n",
    "    logger.debug(f'first tiff:{first_tiff}, n.min:{n.min()}')\n",
    "    logger.debug(f'last tiff:{last_tiff}, n.max:{n.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "start = timeit.default_timer()\n",
    "\n",
    "\n",
    "if lawnPath is not None and lawnPath != 'None':\n",
    "    logger.info('Loading lawn file...')\n",
    "    try:\n",
    "        lawnfile = os.path.join(lawnPath,movieID+'_lawn.tiff')\n",
    "        lawn = pims.open(lawnfile)[0]\n",
    "        binLawn = features.findLawn(lawn)\n",
    "    except Exception:\n",
    "        lawnfile = os.path.join(lawnPath,movieID+'_lawn.bmp')\n",
    "        lawn = pims.open(lawnfile)[0]\n",
    "        binLawn = features.findLawn(lawn)\n",
    "    logger.info(\"Lawnfile opened as 'lawn'\")\n",
    "else:\n",
    "    lawnfile = None\n",
    "\n",
    "\n",
    "\n",
    "# starting pharaglow\n",
    "logger.info(\"Loading tiff files...\")\n",
    "rawframes = pims.open(fname)\n",
    "\n",
    "if debug==0:\n",
    "    rawframes = rawframes\n",
    "if debug==1:\n",
    "    logger.debug(f\"A subset of {len(n)} files will be analyzed\")\n",
    "    rawframes = rawframes[n]\n",
    "logger.info(\"tiff files loaded as 'rawframes'\")\n",
    "\n",
    "\n",
    "logger.info(f\"Loading parameters from {parameterfile}...\")\n",
    "with open(parameterfile) as f:\n",
    "    param = json.load(f)\n",
    "    f.close()\n",
    "logger.info(f\"parameters file loaded as 'param':{param}\")\n",
    "\n",
    "\n",
    "\n",
    "# Measure the wall time for running the current cell[s]\n",
    "stop = timeit.default_timer()\n",
    "logger.info(f\"Loading time: {stop - start}s\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files monitoring\n",
    "\n",
    "# tiff files\n",
    "logger.info(f\"Number of tiff files: {len(os.listdir(inPath))}\")\n",
    "\n",
    "# rawframes \n",
    "logger.info(f\"Number of rawframes: {len(rawframes)}\")\n",
    "\n",
    "if len(os.listdir(inPath)) != len(rawframes):\n",
    "    if not debug:\n",
    "        logger.warning(\"the number of tiff files doesn't match with the number of rawframes !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve lawn detection if neccessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lawnfile is not None:\n",
    "    from skimage.filters import threshold_li, gaussian, threshold_yen, threshold_otsu\n",
    "    from skimage.morphology import skeletonize, watershed, disk, remove_small_holes, remove_small_objects\n",
    "    image = gaussian(lawn, 1, preserve_range = True)\n",
    "    thresh = threshold_li(image, initial_guess = np.median)\n",
    "    binary = image > thresh*0.5\n",
    "    binary = remove_small_holes(binary, area_threshold=1500, connectivity=1, in_place=False)\n",
    "    binary = remove_small_objects(binary, min_size=5000, connectivity=8, in_place=False)\n",
    "    binLawn = binary\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(lawn)\n",
    "    plt.contour(binLawn, alpha=0.5, cmap='pink')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(binLawn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create binary masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# detecting objects\n",
    "logger.info('Binarizing images...')\n",
    "\n",
    "masks = tracking.calculateMask(rawframes,\n",
    "                               minSize = param['minSize'],\n",
    "                               bgWindow = param['bgWindow'],\n",
    "                               thresholdWindow = param['thresholdWindow'],\n",
    "                               smooth =  param['smooth'],\n",
    "                               subtract =  param['subtract'],\n",
    "                               dilate = param['dilate'],\n",
    "                               tfactor=param['tfactor'])\n",
    "\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "logger.info(f\"binary masks created ({stop - start}s)\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure the thresholding worked otherwise change parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a rawframe to visualize\n",
    "t = 300\n",
    "print(f\"rawframe {t} to visualize \")\n",
    "\n",
    "if t> (len(rawframes)-1):\n",
    "    print(f\"Warning ! Max {len(rawframes)-1} rawframes. {t} changed to 0\")\n",
    "    t=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "plt.subplot(121)\n",
    "# Plot the histogram of the pixel intensity values of the rawframe\n",
    "plt.hist(rawframes[t].ravel(), bins=256, log=True)\n",
    "plt.xlim(0, 260) # xlim for 8 bits image\n",
    "\n",
    "plt.subplot(122)\n",
    "# Adjust the color limit for the rawframe for vizualisation only\n",
    "color = (0,60) # 0<=color<=255 for 8 bits image\n",
    "# color = None \n",
    "plt.imshow(rawframes[t],clim = color)\n",
    "plt.colorbar(orientation='horizontal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import label, regionprops\n",
    "\n",
    "plt.figure(figsize=(18,16))\n",
    "\n",
    "plt.subplot(121)\n",
    "# Show the rawframe\n",
    "plt.imshow(rawframes[t],clim= color)#+lawn)\n",
    "if lawnfile is not None:\n",
    "    # Show the lawn\n",
    "    plt.contour(binLawn, alpha=0.5, cmap='pink')\n",
    "    \n",
    "plt.subplot(122)\n",
    "# Show the masks and their size [px]\n",
    "plt.imshow(masks[t])#[:600,1000:])#[500:1500,2000:3500])#[:,2500:])\n",
    "print(np.min(masks[t]))\n",
    "label_image = label(masks[t], background=0, connectivity = 1)\n",
    "for region in regionprops(label_image):\n",
    "    plt.text(region.centroid[1]+50, region.centroid[0], region.area, color ='w')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting individual objects and tracking or use multiprocessing to speed up feature detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "chunksize = 100#\n",
    "logger.info(f'chunksize={chunksize}')\n",
    "\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "chunksize = 100#\n",
    "def parallelDetection(rawframes, masks, nWorkers):\n",
    "    L = len(rawframes)\n",
    "    # create chunks of analysis based on how many workers we use\n",
    "    #slice the movie into pieces to run\n",
    "    slices = zip((range(0,L, chunksize)), (range(chunksize,L+1, chunksize)))\n",
    "    jobs = []\n",
    "    for (a,b) in slices:\n",
    "        print(a,b)\n",
    "        jobs.append([rawframes[a:b+1], masks[a:b+1], param, a])\n",
    "    # add the remainder job for things not divisible by chunksize\n",
    "    jobs.append([rawframes[b:], masks[b:], param, b])\n",
    "    # delete jobs of length 1\n",
    "    jobs = [j for j in jobs if len(j[0])>1]\n",
    "    # add last bit\n",
    "    #run the parallel feature detection.\n",
    "    features = []\n",
    "    images = []\n",
    "    p = Pool(processes = nWorkers)\n",
    "    start = time.time()\n",
    "    for k, res in enumerate(p.imap_unordered(tracking.parallelWorker, jobs)):\n",
    "        features.append(res[0])\n",
    "        images.append(res[1])\n",
    "        if k == nWorkers:\n",
    "            print('Expected time is approx. {} s'.format((L/chunksize-k)*(time.time()-start)/nWorkers))\n",
    "    \n",
    "    p.close()\n",
    "    p.join()\n",
    "    features = pd.concat(features)\n",
    "    images = pd.concat(images)\n",
    "    # change the image column names\n",
    "    images.columns = [f\"im{s}\" for s in images.columns]\n",
    "    # make one big dataframe\n",
    "    features = pd.concat([features, images], axis = 1)\n",
    "    features = features.reset_index(drop=True)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "start = timeit.default_timer()\n",
    "\n",
    "logger.info('Detecting features...')\n",
    "\n",
    "if nWorkers ==1 or len(rawframes) < chunksize:\n",
    "    logger.info('...without parallel detection...')\n",
    "    features, images = tracking.runfeatureDetection(rawframes, masks, param, frameOffset = 0)\n",
    "    features = pd.concat([features, images], axis = 1)\n",
    "else:\n",
    "    logger.info('...with parallel detection...')\n",
    "    features = parallelDetection(rawframes, masks, nWorkers)\n",
    "    \n",
    "\n",
    "stop = timeit.default_timer()\n",
    "logger.info(f\"features detected ({stop - start}s)\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files monitoring\n",
    "logger.info(f\" Number of frames in features:{features['frame'].nunique()}\")\n",
    "                                                       \n",
    "if len(rawframes) != len(features['frame'].unique()):\n",
    "    logger.warning(f\" Number of frames in features ({features['frame'].nunique()}) and the number of rawframes ({len(rawframes)}) don't match !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show the area of all objects\n",
    "\n",
    "features['area'].hist(bins = 30)\n",
    "\n",
    "logger.info(f\"features.area.min():{features.area.min()}\") # region.area > params['minSize']\n",
    "logger.info(f\"features.area.max():{features.area.max()}\") # region.area < params['maxSize']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving features\n",
    "logger.info(\"Saving features...\")\n",
    "features.info(memory_usage='deep')\n",
    "features.to_json(outfile.format('features', 'all'), orient='split')\n",
    "logger.info(f\"features saved as {outfile.format('features', 'all')}\")\n",
    "\n",
    "# saving parameter file\n",
    "logger.info(\"Saving parameters...\")\n",
    "shutil.copyfile(parameterfile, saveparam, follow_symlinks=True)\n",
    "logger.info(f\"parameters saved as {parameterfile}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all features \n",
    "-- only if execution was was interrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "start = timeit.default_timer()\n",
    "\n",
    "logger.info(\"Loading features...\")\n",
    "features = pd.read_json(outfile.format('features', 'all'), orient='split')\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "logger.info(f\"features loaded ({stop - start}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test the new image storageq\n",
    "N=25\n",
    "colnames = [f'im{i}' for i in range(int(features.iloc[N]['shapeX'])*int(features.iloc[N]['shapeY']))]\n",
    "tim = util.get_im(features.iloc[N], colnames, int(features.iloc[N]['shapeX']))\n",
    "\n",
    "plt.imshow(tim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link objects to trajectories and interpolate short misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Linking trajectories...')\n",
    "logger.info(f\"Parameter searchRange: {param['searchRange']}px\")\n",
    "logger.info(f\"Parameter memory: {param['memory']}frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred = tp.predict.NearestVelocityPredict()\n",
    "#trajectories = pred.link_df(features,param['searchRange'], memory = param['memory'])\n",
    "trajectories = tp.link_df(features,param['searchRange'], memory = param['memory'])\n",
    "logger.info(f\"Number of trajectories after linking: {len(trajectories['particle'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Interpolating trajectories...')\n",
    "\n",
    "traj = []\n",
    "\n",
    "for particle_index in trajectories['particle'].unique():\n",
    "    tmp = trajectories[trajectories.loc[:,'particle'] == particle_index]\n",
    "    # interpolate data but do not interpolate the images!\n",
    "    traj.append(tracking.interpolateTrajectories(tmp, columns = ['x', 'y', 'shapeX', 'shapeY', 'particle']))\n",
    "    \n",
    "trajectories = pd.concat(traj, ignore_index = True)\n",
    "trajectories['shapeX'] = trajectories['shapeX'].astype(int)\n",
    "trajectories['shapeY'] = trajectories['shapeY'].astype(int)\n",
    "\n",
    "logger.info(f\"Interpolation done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "tp.plot_traj(trajectories, colorby = 'particle')#, superimpose=1-masks[t]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Filtering out trajectories which last less than the minimal duration...\")\n",
    "logger.info(f\"Parameter 'minimal duration' = {param['minimalDuration']} frames\")\n",
    "logger.info(f\"Nb of trajectories before filtering: {trajectories['particle'].nunique()}\")\n",
    "\n",
    "trajectories = tp.filter_stubs(trajectories,param['minimalDuration'])\n",
    "logger.info(f\"Nb of trajectories after filtering: {trajectories['particle'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "tp.plot_traj(trajectories, superimpose=1-masks[t]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add the missing images to interpolated trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "start = timeit.default_timer()\n",
    "\n",
    "\n",
    "from pharaglow import tracking, run, features\n",
    "# interpolate the shape parameter\n",
    "def interpolate_helper(rawframes, row):\n",
    "    \"\"\"wrapper to make the code more readable.\"\"\"\n",
    "    # check if image is nan - then we insert an image from the original movie\n",
    "    im_cols =  [f'im{i}' for i in range(int(row['shapeX'])*int(row['shapeY']))]\n",
    "    \n",
    "    if np.all(np.isnan(row[im_cols])):\n",
    "        print('interpolating', row['frame'])\n",
    "        im, sx0,sx1,sy0, sy1, ly, lx = tracking.fillMissingImages(rawframes, int(row['frame']), row['x'], row['y'],\\\n",
    "                                                   lengthX=row['shapeX'],lengthY=row['shapeY'], size=param['watershed'])\n",
    "        # make the image into a pandas format and return a whole row\n",
    "        im_cols =  [f'im{i}' for i in range(lx*ly)]\n",
    "        # other column names\n",
    "        [im_cols.append(x) for x in ['slice_x0','slice_x1','slice_y0','slice_y1', 'shapeY', 'shapeX']]\n",
    "        row[im_cols] = [*list(im.ravel()),  sx0,sx1,sy0, sy1, ly, lx]\n",
    "    return row\n",
    "\n",
    "logger.info('Fill in missing images...')\n",
    "trajectories = trajectories.apply(\\\n",
    "    lambda row: interpolate_helper(rawframes, row), axis=1)\n",
    "\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "logger.info(f\"Missing images added to interpolated trajectories ({stop - start}s)\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract lawn info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def inside(x,y,binLawn):\n",
    "    return binLawn[int(y), int(x)]\n",
    "\n",
    "if lawnfile is not None:\n",
    "    trajectories['inside'] = trajectories.apply(\\\n",
    "        lambda row: pd.Series(inside(row['x'], row['y'], binLawn)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show resulting trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,11))\n",
    "plt.title(f\"{trajectories['particle'].nunique()} trajectories detected ({movieID})\")\n",
    "tp.plot_traj(trajectories)#, superimpose=1-masks[t]);\n",
    "\n",
    "plt.savefig(os.path.join(outPath,f'{today}_{movieID}_resulting_trajectories.pdf'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write trajectories to separate files.\n",
    "\n",
    "logger.info(f\"Saving {trajectories['particle'].nunique()} trajectories to separate files...\")\n",
    "\n",
    "for particle_index in trajectories['particle'].unique():\n",
    "    tmp = trajectories[trajectories.loc[:,'particle'] == particle_index]\n",
    "    \n",
    "    # write trajectories to file\n",
    "    tmp.to_json(outfile.format('trajectories', int(particle_index)), orient='split')\n",
    "    \n",
    "logger.info(\"Trajectories saved as json files.\") \n",
    "\n",
    "# # Write trajectories to a single file\n",
    "# trajectories = pd.read_json(outfile.format('trajectories', 'all'), orient='split', numpy = True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check slow-down before continuing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lawnfile is not None:\n",
    "    plt.figure(figsize=(12,8))\n",
    "    vcut = []\n",
    "    dt = 1000\n",
    "    for pid in trajectories['particle'].unique():\n",
    "        tmp = trajectories[['frame', 'x', 'y']][trajectories.loc[:,'particle'] == pid].diff()\n",
    "        f = (trajectories[['inside']][trajectories.loc[:,'particle'] == pid]).mean().values\n",
    "        if f<0.9 and f>0.01:\n",
    "            t0 = np.where((trajectories[['inside']][trajectories.loc[:,'particle'] == pid])==1)[0][0]\n",
    "            print('t0:', t0)\n",
    "            try:\n",
    "                if t0>dt:\n",
    "                    print('pid:', pid)\n",
    "                    time = np.linspace(0,2*dt/30., 2*dt)\n",
    "                    #print('time:', len(time))\n",
    "                    v = np.sqrt((tmp['x']**2+tmp['y']**2))/tmp['frame']*30*2.4\n",
    "                    #print('v:', v)\n",
    "                    #print('v.iloc:', v.iloc[t0-dt:t0+dt])\n",
    "                    plt.plot(time, v.iloc[t0-dt:t0+dt], 'navy', alpha=0.1)\n",
    "                    vcut.append(v.iloc[t0-dt:t0+dt].values)\n",
    "                else:\n",
    "                    print('trajectory is too short')\n",
    "            except ValueError:\n",
    "                print('t0-dt or t0+dt exceeds number of frames')\n",
    "                continue\n",
    "                    \n",
    "    if len(vcut) >0:  \n",
    "        plt.plot(time, np.mean(np.array(vcut), axis=0), color='navy')\n",
    "        plt.plot(time, util.smooth(np.mean(np.array(vcut), axis=0), 30), color='r')\n",
    "        plt.axvline(dt/30, color='k', linestyle='--')\n",
    "        plt.ylabel(r\"velocity ($\\mu$m/s)\");\n",
    "        plt.xlabel(\"time (s)\");\n",
    "        plt.ylim(0,150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run the whole pharaglow feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "def parallelize_dataframe(df, func, params, n_cores):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    df_split = [d for d in df_split  if len(d)>0]\n",
    "    if len(df_split) <1:\n",
    "        return\n",
    "    # filter zero-size jobs\n",
    "    print([len(d) for d in df_split])\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.starmap(func, zip(df_split, np.repeat(params, len(df_split)))))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# save only minimal outputs - reduces save by approx factor 3\n",
    "save_minimal = True\n",
    "path = os.path.dirname(outfile)\n",
    "\n",
    "for fn in os.listdir(path):\n",
    "    file = os.path.join(path,fn)\n",
    "    if os.path.isfile(file) and f'{movieID}_trajectories_' in fn and fn.endswith('.json'):\n",
    "        particle_index = int(fn.split('.')[0].split('_')[-1])\n",
    "        traj =  pd.read_json(file, orient='split')\n",
    "        # create a temporary column with an image array\n",
    "        traj['image'] = traj.apply(lambda row: util.get_im(row, [f'im{i}' for i in range(int(row['shapeX'])*int(row['shapeY']))],\n",
    "                                            int(row['shapeX'])), axis=1)\n",
    "        \n",
    "        # skip invalid frames\n",
    "        invalid_images = traj[[np.sum(im)==0 for im in traj['image']]].index\n",
    "        if len(invalid_images)>0:\n",
    "            logger.info(f'invalid images in frames {invalid_images}')\n",
    "        # ignore rows where images are empty - we later will interpolate\n",
    "        traj = traj.drop(invalid_images)\n",
    "        if len(traj.index)<1:\n",
    "            print('Skipped', file)\n",
    "            continue\n",
    "        traj['shapeX'] = traj['shapeX'].astype(int)\n",
    "        print('Analyzing trajectory:', fn)\n",
    "        tmp = parallelize_dataframe(traj, run.runPharaglowOnStack, n_cores = nWorkers, params = param)\n",
    "        \n",
    "        # get more exact entry location\n",
    "        if lawnfile is not None:\n",
    "            tmp['insideHead'] = tmp.apply(\\\n",
    "                lambda row: pd.Series(features.headLocationLawn(row['Centerline'],row['slice'], binLawn)), axis=1)\n",
    "            tmp['insideHeadIntensity'] = tmp.apply(\\\n",
    "                lambda row: pd.Series(features.headLocationLawn(row['Centerline'],row['slice'], lawn)), axis=1)\n",
    "        \n",
    "        # remove some columns to make the result smaller\n",
    "        if save_minimal:\n",
    "            tmp = tmp.drop(['Mask', 'SkeletonX', 'SkeletonY', 'ParX', 'ParY', \n",
    "                            'Xstart', 'Xend', 'Centerline', 'dCl', 'Widths', 'Contour', 'Gradient', \n",
    "                            'Kymo', 'KymoGrad', 'Similarity', 'Xtmp'], axis = 1)\n",
    "        \n",
    "        tmp = tmp.drop(['image'], axis = 1)\n",
    "        tmp.to_json(outfile.format('results', particle_index), orient='split')\n",
    "\n",
    "if save_minimal:\n",
    "    logger.info('minimal information saved')\n",
    "    \n",
    "stop = timeit.default_timer()\n",
    "logger.info(f\"Whole pharaglow features extracted  ({stop - start}s)\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if data has been analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files monitoring\n",
    "\n",
    "files_list = os.listdir(outPath)\n",
    "f1 =[]\n",
    "f2 =[]\n",
    "\n",
    "for fn in files_list:\n",
    "    file = os.path.join(outPath,fn)\n",
    "    if os.path.isfile(file) and f'{movieID}_trajectories_' in fn  and fn.endswith('.json'):\n",
    "        if not 'all' in fn: \n",
    "            particle_index = int(fn.split('.')[0].split('_')[-1])\n",
    "            f1.append(particle_index)\n",
    "    if os.path.isfile(file) and f'{movieID}_results_' in fn and fn.endswith('.json'): \n",
    "        particle_index = int(fn.split('.')[0].split('_')[-1])\n",
    "        f2.append(particle_index)\n",
    "\n",
    "\n",
    "logger.info('trajectories.json files: %s ', len(f1))\n",
    "logger.info('results.json files: %s ', len(f2))\n",
    "if len(f1) != len(f2):\n",
    "    logger.warning('trajectories - results: %s', set(f1).symmetric_difference(set(f2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving parameters if they have been changed (debug mode activated)\n",
    "if debug:\n",
    "    logger.debug(f\"New parameters:{param}\")\n",
    "    paramPath = os.path.join(outPath, movieID + '_parameters_new.txt')\n",
    "    with open(paramPath,'w') as f:\n",
    "         f.write(json.dumps(param)) # use `json.loads` to do the reverse\n",
    "         \n",
    "    logger.debug(f\"New parameters saved as {paramPath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"PharaGlow ends here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (basic)",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
